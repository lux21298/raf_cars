import os
import json
import chromadb
import requests

# --- Constants and Configuration ---
# ThÆ° má»¥c Ä‘á»ƒ lÆ°u trá»¯ cÆ¡ sá»Ÿ dá»¯ liá»‡u vector ChromaDB
CHROMA_DIR = "chroma_db_cars" # Change directory name to avoid conflicts with other projects
# TÃªn cá»§a bá»™ sÆ°u táº­p (collection) trong ChromaDB Ä‘á»ƒ lÆ°u trá»¯ car data
COLLECTION_NAME = "cars"
# TÃªn cá»§a tá»‡p JSON chá»©a dá»¯ liá»‡u ban Ä‘áº§u vá» car
JSON_FILE = "cars.json"
# TÃªn cá»§a mÃ´ hÃ¬nh nhÃºng (embedding model) Ä‘Æ°á»£c sá»­ dá»¥ng thÃ´ng qua Ollama
EMBED_MODEL = "mxbai-embed-large"
# TÃªn cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘Æ°á»£c sá»­ dá»¥ng thÃ´ng qua Ollama Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i
LLM_MODEL = "llama3.2"

# --- Load initial data ---
# Má»Ÿ tá»‡p JSON chá»©a car data vÃ  táº£i ná»™i dung vÃ o biáº¿n car_data
with open(JSON_FILE, "r", encoding="utf-8") as f:
    car_data = json.load(f)

# --- ChromaDB Setup ---
# Khá»Ÿi táº¡o má»™t ChromaDB client. PersistentClient Ä‘áº£m báº£o dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn Ä‘Ä©a.
chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)
# Láº¥y hoáº·c táº¡o má»™t collection trong ChromaDB.
# Náº¿u collection 'cars' khÃ´ng tá»“n táº¡i, nÃ³ sáº½ Ä‘Æ°á»£c táº¡o.
collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)

# --- HÃ m táº¡o Embeddings vá»›i Ollama ---
# HÃ m nÃ y giao tiáº¿p vá»›i API Ollama Ä‘á»ƒ táº¡o báº£n nhÃºng (embedding) cho má»™t Ä‘oáº¡n vÄƒn báº£n.
def get_embedding(text):
    try:
        # Gá»­i má»™t POST request Ä‘áº¿n endpoint /api/embeddings cá»§a Ollama
        response = requests.post("http://localhost:11434/api/embeddings", json={
            "model": EMBED_MODEL,  # Chá»‰ Ä‘á»‹nh embedding model Ä‘á»ƒ sá»­ dá»¥ng
            "prompt": text         # VÄƒn báº£n Ä‘á»ƒ táº¡o embedding
        })
        response.raise_for_status() # Kiá»ƒm tra lá»—i HTTP
        data = response.json()
        # print("Embedding API response:", data)  # Uncomment this line for debugging if needed
        if "embedding" not in data:
            raise ValueError(f"Embedding API error: 'embedding' key missing in response: {data}")
        # Return the embedding vector
        return data["embedding"]
    except requests.exceptions.ConnectionError:
        print("ğŸš« Connection error to Ollama. Please ensure Ollama is running.")
        print("   You can check by opening Terminal/Command Prompt and typing: ollama run llama3.2")
        exit() # Exit the program if connection fails
    except Exception as e:
        print(f"ğŸš« Error getting embedding: {e}")
        exit() # Exit the program if another error occurs

# --- Add new data to ChromaDB (if any) ---
# Láº¥y táº¥t cáº£ cÃ¡c ID tÃ i liá»‡u hiá»‡n cÃ³ trong ChromaDB collection
# Sá»­ dá»¥ng try-except Ä‘á»ƒ xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p collection trá»‘ng hoáº·c cÃ³ lá»—i vá»›i get()
try:
    existing_ids = set(collection.get(ids=[item['id'] for item in car_data])['ids'])
except:
    existing_ids = set() # Náº¿u cÃ³ lá»—i hoáº·c collection trá»‘ng, giáº£ sá»­ khÃ´ng cÃ³ ID nÃ o tá»“n táº¡i

# Lá»c cÃ¡c má»¥c má»›i tá»« car_data chÆ°a cÃ³ trong ChromaDB
new_items = [item for item in car_data if item['id'] not in existing_ids]

# Kiá»ƒm tra xem cÃ³ báº¥t ká»³ má»¥c má»›i nÃ o Ä‘á»ƒ thÃªm khÃ´ng
if new_items:
    print(f"ğŸ†• Adding {len(new_items)} new documents to ChromaDB...")
    # Láº·p láº¡i tá»«ng má»¥c má»›i
    for item in new_items:
        # Báº¯t Ä‘áº§u vá»›i mÃ´ táº£ vÄƒn báº£n gá»‘c cá»§a car
        enriched_text = item["text"]
        # LÃ m phong phÃº vÄƒn báº£n báº±ng cÃ¡ch thÃªm thÃ´ng tin make náº¿u cÃ³
        if "make" in item:
            enriched_text += f" This car is made by {item['make']}."
        # LÃ m phong phÃº vÄƒn báº£n báº±ng cÃ¡ch thÃªm thÃ´ng tin car type náº¿u cÃ³
        if "type" in item:
            enriched_text += f" It is a type of {item['type']}."
        # ThÃªm thÃ´ng tin sá»‘ chá»— ngá»“i náº¿u cÃ³
        if "seat" in item:
            enriched_text += f" It has {item['seat']} seats."
        # ThÃªm thÃ´ng tin loáº¡i nhiÃªn liá»‡u náº¿u cÃ³
        if "fuel_type" in item:
            enriched_text += f" It uses {item['fuel_type']} fuel."

        # Táº¡o má»™t embedding cho vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c lÃ m phong phÃº
        emb = get_embedding(enriched_text)

        # ThÃªm tÃ i liá»‡u vÃ o ChromaDB collection
        # documents: VÄƒn báº£n thá»±c táº¿ sáº½ Ä‘Æ°á»£c truy xuáº¥t lÃ m context
        # embeddings: Embedding Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¬m kiáº¿m sá»± tÆ°Æ¡ng Ä‘á»“ng
        # ids: ID duy nháº¥t cá»§a tÃ i liá»‡u
        collection.add(
            documents=[item["text"]],  # Sá»­ dá»¥ng vÄƒn báº£n gá»‘c lÃ m context cÃ³ thá»ƒ truy xuáº¥t
            embeddings=[emb],          # Sá»­ dá»¥ng embedding cá»§a vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c lÃ m phong phÃº Ä‘á»ƒ tÃ¬m kiáº¿m
            ids=[item["id"]]           # ID cá»§a car
        )
    print("âœ… New documents added successfully.")
else:
    print("âœ… All documents already in ChromaDB.")

# --- RAG Query Function (Retrieval-Augmented Generation) ---
# HÃ m nÃ y thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh RAG Ä‘á»ƒ tráº£ lá»i má»™t cÃ¢u há»i
def rag_query(question):
    # Step 1: Embed the user question
    # Chuyá»ƒn Ä‘á»•i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng thÃ nh má»™t embedding vector
    q_emb = get_embedding(question)

    # Step 2: Query the vector database (ChromaDB)
    # TÃ¬m kiáº¿m cÃ¡c tÃ i liá»‡u tÆ°Æ¡ng tá»± nháº¥t vá»›i embedding cá»§a cÃ¢u há»i.
    # n_results=3: Tráº£ vá» 3 tÃ i liá»‡u liÃªn quan nháº¥t.
    results = collection.query(query_embeddings=[q_emb], n_results=3)

    # Step 3: Extract relevant documents
    # Láº¥y ná»™i dung vÄƒn báº£n cá»§a cÃ¡c tÃ i liá»‡u hÃ ng Ä‘áº§u
    top_docs = results['documents'][0]
    # Láº¥y ID cá»§a cÃ¡c tÃ i liá»‡u hÃ ng Ä‘áº§u
    top_ids = results['ids'][0]

    # Step 4: Display friendly explanation of retrieved documents
    print("\nğŸ§  Retrieving relevant information to answer your question...\n")
    for i, doc in enumerate(top_docs):
        print(f"ğŸ”¹ Source {i + 1} (ID: {top_ids[i]}):")
        print(f"    \"{doc}\"\n")
    print("ğŸ“š These seem to be the most relevant pieces of information to answer your question.\n")

    # Step 5: Build the prompt from the retrieved context
    # Ná»‘i cÃ¡c tÃ i liá»‡u Ä‘Ã£ truy xuáº¥t thÃ nh má»™t chuá»—i context duy nháº¥t
    context = "\n".join(top_docs)

    # Táº¡o prompt cho LLM. Prompt bao gá»“m context vÃ  cÃ¢u há»i,
    # hÆ°á»›ng dáº«n LLM chá»‰ sá»­ dá»¥ng context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i.
    prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {question}
Answer:"""

    # Step 6: Generate answer with Ollama (using the LLM)
    try:
        # Gá»­i má»™t POST request Ä‘áº¿n endpoint /api/generate cá»§a Ollama
        response = requests.post("http://localhost:11434/api/generate", json={
            "model": LLM_MODEL,  # Chá»‰ Ä‘á»‹nh LLM model Ä‘á»ƒ sá»­ dá»¥ng
            "prompt": prompt,    # Prompt Ä‘Ã£ Ä‘Æ°á»£c tÄƒng cÆ°á»ng context
            "stream": False      # KhÃ´ng stream pháº£n há»“i, Ä‘á»£i pháº£n há»“i Ä‘áº§y Ä‘á»§
        })
        response.raise_for_status() # Kiá»ƒm tra lá»—i HTTP
        # Step 7: Return the final result
        # TrÃ­ch xuáº¥t cÃ¢u tráº£ lá»i tá»« pháº£n há»“i JSON vÃ  loáº¡i bá» khoáº£ng tráº¯ng Ä‘áº§u/cuá»‘i
        return response.json()["response"].strip()
    except requests.exceptions.ConnectionError:
        return "ğŸš« Connection error to Ollama when generating answer. Please ensure Ollama is running."
    except Exception as e:
        return f"ğŸš« Error generating answer: {e}"


# --- Interactive Loop ---
print("\nğŸ§  Car RAG is ready. Ask a question (type 'exit' to quit):\n")
while True:
    # Láº¥y cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng
    question = input("You: ")
    # ThoÃ¡t náº¿u ngÆ°á»i dÃ¹ng nháº­p 'exit' hoáº·c 'quit'
    if question.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ Goodbye!")
        break
    # Gá»i hÃ m rag_query Ä‘á»ƒ láº¥y cÃ¢u tráº£ lá»i
    answer = rag_query(question)
    # In cÃ¢u tráº£ lá»i tá»« LLM
    print("ğŸ¤–:", answer)
